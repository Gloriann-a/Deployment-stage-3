 DevOps Stage 3 â€“ Observability & Alerts Runbook

This document explains how to interpret alerts generated by the alert_watcher and the recommended operator actions.

 Overview

The observability setup consists of:

Nginx proxy with structured access logs (custom_json format)

alert_watcher (Python container) that tails logs in real-time

Slack alerts for failover events and elevated upstream error rates


All configuration values are stored in .env and can be tuned without changing code.

 Key Environment Variables

Variable	Description	Example

SLACK_WEBHOOK_URL	Slack incoming webhook URL	https://hooks.slack.com/services/...
ACTIVE_POOL	Current traffic pool (blue or green)	green
ERROR_RATE_THRESHOLD	% of 5xx errors that triggers alert	2
WINDOW_SIZE	Number of requests to analyze	200
ALERT_COOLDOWN_SEC	Minimum seconds between alerts	300
MAINTENANCE_MODE	Suppress alerts during planned toggles	false



ðŸš¨ Alert Types and Actions

1. Failover Detected

Slack Message Example:

:arrows_counterclockwise: Failover detected! blue â†’ green

Meaning:
Traffic has switched from one pool to another â€” e.g., the blue deployment failed or was manually toggled off.

Operator Actions:

i. Run docker ps to confirm which container is healthy.


ii. Check Nginx and application logs (docker logs nginx, docker logs app_blue/app_green).


iii. If unplanned, verify that the inactive pool container is reachable and restart it if needed.


iv. If planned (e.g., during maintenance), ignore the alert or set MAINTENANCE_MODE=true beforehand.




2. High Error Rate Detected

Slack Message Example:

:rotating_light: High error rate detected! (3.5% over last 200 requests)

Meaning:
The watcher detected an increase in 5xx errors across recent traffic, suggesting instability or upstream failure.

Operator Actions:

i. Run docker logs nginx to identify the affected upstream.


ii. Inspect app logs to find the cause of 5xx errors.


iii. If one pool is unhealthy, toggle to the other pool using:

export ACTIVE_POOL=blue   # or green
docker compose up -d nginx


iv. Monitor Slack for recovery alerts.


3. Recovery Event (Optional)

If you choose to extend the watcher with recovery detection, you may see messages like:

:white_check_mark: Primary pool recovered and serving traffic again.

Operator Actions:

Verify that error rates have stabilized.

Keep monitoring for a few minutes before resuming normal operations.



Maintenance Mode

To prevent spam during planned deployments or chaos tests:

i. Set MAINTENANCE_MODE=true in .env


ii. Run:

docker compose up -d


iii. When done, revert it back to false.


 Log Verification

To view logs locally:

docker compose logs nginx | grep pool

Each log entry should contain structured fields:

{
  "time": "30/Oct/2025:16:55:21 +0000",
  "status": "200",
  "pool": "green",
  "release": "v2.0.0",
  "upstream_status": "200",
  "upstream_addr": "app_green:3000"
}


Submission Checklist

[x] docker-compose.yml includes alert_watcher

[x] nginx.conf.template logs contain pool, release, upstream_status

[x] .env.example has all variables

[x] watcher.py + requirements.txt + Dockerfile

[x] runbook.md (this file)

[x] Slack screenshots for failover and error alerts

